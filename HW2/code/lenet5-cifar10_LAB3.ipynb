{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training LeNet-5 on CIFAR-10\n",
    "In this notebook, we will try to deploy the famous LeNet-5 to solve a simple image classification task, the CIFAR-10. CIFAR-10 is composed of 60K images from 10 categories. After splitting the dataset, we have 45K/5K/10K images for train/valiation/test dataset.\n",
    "In this notebook, only the labels of training/validation dataset is visible to you, so you can use the training and validation data to tune your model. After you submitted your model, your final grade will be determined on the model performance on the holdout test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0 Setting up LeNet-5 model\n",
    "As you have set up the LeNet-5 model in Homework 1, we will just move the implementation of LeNet-5 model here, so you can use it for this homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-15T04:46:22.231275Z",
     "start_time": "2020-09-15T04:46:19.511325Z"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os, sys\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# Import pytorch dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-15T04:46:22.248192Z",
     "start_time": "2020-09-15T04:46:22.233794Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def swish(x):\n",
    "    return x * torch.sigmoid(x)\n",
    "# Create the neural network module: LeNet-5\n",
    "class ResNet20(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet20, self).__init__()\n",
    "        \n",
    "\n",
    "        self.conv0 = nn.Conv2d(3, 16, 3)\n",
    "        self.norm0 = nn.BatchNorm2d(16)\n",
    "\n",
    "\n",
    "        self.conv11 = nn.Conv2d(16, 16, 3,padding=1)\n",
    "        self.norm11 = nn.BatchNorm2d(16)\n",
    "        self.conv12 = nn.Conv2d(16, 16, 3,padding=1)\n",
    "        self.norm12 = nn.BatchNorm2d(16)\n",
    "        self.conv13 = nn.Conv2d(16, 16, 3,padding=1)\n",
    "        self.norm13 = nn.BatchNorm2d(16)\n",
    "        self.conv14 = nn.Conv2d(16, 16, 3,padding=1)\n",
    "        self.norm14 = nn.BatchNorm2d(16)\n",
    "        self.conv15 = nn.Conv2d(16, 16, 3,padding=1)\n",
    "        self.norm15 = nn.BatchNorm2d(16)\n",
    "        self.conv16 = nn.Conv2d(16, 16, 3,padding=1)\n",
    "        self.norm16 = nn.BatchNorm2d(16)\n",
    "\n",
    "        self.con1to2=nn.Conv2d(16, 32, 1,stride=2) \n",
    "        self.norm1to2 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.conv21 = nn.Conv2d(16, 32, 3,padding=1,stride=2)\n",
    "        self.norm21 = nn.BatchNorm2d(32)\n",
    "        self.conv22 = nn.Conv2d(32, 32, 3,padding=1)\n",
    "        self.norm22 = nn.BatchNorm2d(32)\n",
    "        self.conv23 = nn.Conv2d(32, 32, 3,padding=1)\n",
    "        self.norm23 = nn.BatchNorm2d(32)\n",
    "        self.conv24 = nn.Conv2d(32, 32, 3,padding=1)\n",
    "        self.norm24 = nn.BatchNorm2d(32)\n",
    "        self.conv25 = nn.Conv2d(32, 32, 3,padding=1)\n",
    "        self.norm25 = nn.BatchNorm2d(32)\n",
    "        self.conv26 = nn.Conv2d(32, 32, 3,padding=1)\n",
    "        self.norm26 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.con2to3=nn.Conv2d(32, 64, 1,stride=2) \n",
    "        self.norm2to3 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv31 = nn.Conv2d(32, 64, 3,padding=1,stride=2)\n",
    "        self.norm31 = nn.BatchNorm2d(64)\n",
    "        self.conv32 = nn.Conv2d(64, 64, 3,padding=1)\n",
    "        self.norm32 = nn.BatchNorm2d(64)\n",
    "        self.conv33 = nn.Conv2d(64, 64, 3,padding=1)\n",
    "        self.norm33 = nn.BatchNorm2d(64)\n",
    "        self.conv34 = nn.Conv2d(64, 64, 3,padding=1)\n",
    "        self.norm34 = nn.BatchNorm2d(64)\n",
    "        self.conv35 = nn.Conv2d(64, 64, 3,padding=1)\n",
    "        self.norm35 = nn.BatchNorm2d(64)\n",
    "        self.conv36 = nn.Conv2d(64, 64, 3,padding=1)\n",
    "        self.norm36 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.avePool=nn.AvgPool2d(8) \n",
    "        \n",
    "        self.fc   = nn.Linear(64, 10)\n",
    "\n",
    "        # self.softmax=nn.Softmax()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        out0 = F.relu(self.conv0(x))\n",
    "        out0 = self.norm0(out0)\n",
    "\n",
    "        out11=F.relu(self.conv11(out0))\n",
    "        out11=self.norm11(out11)\n",
    "        out12=F.relu(self.conv12(out11))\n",
    "        out12=self.norm12(out12)\n",
    "        out13=F.relu(self.conv13(out12+out0))\n",
    "        out13=self.norm13(out13)\n",
    "        out14=F.relu(self.conv14(out13))\n",
    "        out14=self.norm14(out14)\n",
    "        out15=F.relu(self.conv15(out14+out12))\n",
    "        out15=self.norm15(out15)\n",
    "        out16=F.relu(self.conv16(out15))\n",
    "        out16=self.norm16(out16)\n",
    "\n",
    "\n",
    "        out21=F.relu(self.conv21(out16+out14))\n",
    "        out21=self.norm21(out21)\n",
    "        out22=F.relu(self.conv22(out21))\n",
    "        out22=self.norm22(out22)\n",
    "        out16t=self.con1to2(out16)\n",
    "        out16t=self.norm1to2(out16t)\n",
    "        out23=F.relu(self.conv23(out22+out16t))\n",
    "        out23=self.norm23(out23)\n",
    "        out24=F.relu(self.conv24(out23))\n",
    "        out24=self.norm24(out24)\n",
    "        out25=F.relu(self.conv25(out24+out22))\n",
    "        out25=self.norm25(out25)\n",
    "        out26=F.relu(self.conv26(out25))\n",
    "        out26=self.norm26(out26)\n",
    "\n",
    "        out31=F.relu(self.conv31(out26+out24))\n",
    "        out31=self.norm31(out31)\n",
    "        out32=F.relu(self.conv32(out31))\n",
    "        out32=self.norm32(out32)\n",
    "        out26t=self.con2to3(out26)\n",
    "        out26t=self.norm2to3(out26t)\n",
    "        out33=F.relu(self.conv33(out32+out26t))\n",
    "        out33=self.norm33(out33)\n",
    "        out34=F.relu(self.conv34(out33))\n",
    "        out34=self.norm34(out34)\n",
    "        out35=F.relu(self.conv35(out34+out32))\n",
    "        out35=self.norm35(out35)\n",
    "        out36=F.relu(self.conv36(out35))\n",
    "        out36=self.norm36(out36)+out34\n",
    "\n",
    "        out=self.avePool(out36)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out=self.fc(out)\n",
    "        # out=self.softmax(out)\n",
    "       \n",
    "        return out\n",
    "\n",
    "    \"\"\"\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * (m.in_channels + m.out_channels)\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                n = m.weight.size(1)\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_() \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Setting up preprocessing functions.\n",
    "Preprocessing is very important because it prepares your data for proceeding training steps.\n",
    "Write functions to load dataset and preprocess the incoming data. We recommend that the preprocess scheme \\textbf{must} include normalize, standardization, batch shuffling to make sure the training \n",
    "process goes smoothly. The preprocess scheme may also contain some data augmentation methods \n",
    "(e.g., random crop, random flip, etc.). \n",
    "\n",
    "Reference value for mean/std:\n",
    "\n",
    "**mean(RGB-format): (0.4914, 0.4822, 0.4465)**\n",
    "\n",
    "**std(RGB-format): (0.2023, 0.1994, 0.2010)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-15T04:46:22.266974Z",
     "start_time": "2020-09-15T04:46:22.257233Z"
    }
   },
   "outputs": [],
   "source": [
    "# Specify preprocessing function.\n",
    "# Reference mean/std value for \n",
    "transform_train  = transforms.Compose([\n",
    "          transforms.RandomCrop(size=[32,32],padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "       transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "        transforms.RandomCrop(size=[32,32],padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Setting up data I/O\n",
    "Data I/O reads data from the dataset and prepares it for further procedures. Note that you have to link transformation with data I/O so that these operations can be interleaved. Thus, the training process can be more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-15T04:46:26.844376Z",
     "start_time": "2020-09-15T04:46:22.274174Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Using downloaded and verified file: ./data/cifar10_trainval_F20.zip\nExtracting ./data/cifar10_trainval_F20.zip to ./data\nFiles already downloaded and verified\nTraining dataset has 45000 examples!\nUsing downloaded and verified file: ./data/cifar10_trainval_F20.zip\nExtracting ./data/cifar10_trainval_F20.zip to ./data\nFiles already downloaded and verified\nValidation dataset has 5000 examples!\n"
    }
   ],
   "source": [
    "# You cannot change this line.\n",
    "from tools.dataloader import CIFAR10\n",
    "# Call the dataset Loader\n",
    "DATAROOT = \"./data\"\n",
    "TRAIN_BATCH_SIZE = 128\n",
    "VAL_BATCH_SIZE = 100\n",
    "trainset = CIFAR10(root=DATAROOT, train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "valset = CIFAR10(root=DATAROOT, train=False, download=True, transform=transform_val)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=VAL_BATCH_SIZE, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Instantialize your LeNet-5 model and deploy it to GPU devices.\n",
    "You may want to deploy your model to GPU device for efficient training. Please assign your model to GPU if possible. If you are training on a machine without GPUs, please deploy your model to CPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-15T04:46:26.861182Z",
     "start_time": "2020-09-15T04:46:26.846976Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on GPU...\n"
    }
   ],
   "source": [
    "# Specify the device for computation\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "net = ResNet20()\n",
    "net = net.to(device)\n",
    "if device =='cuda':\n",
    "    print(\"Train on GPU...\")\n",
    "else:\n",
    "    print(\"Train on CPU...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter settings\n",
    "Hyperparameters are quite crucial in determining the performance of our model. The default hyperparameter settings are sufficient for a decent result. You may tune them wisely and carefully for better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-15T04:46:26.880428Z",
     "start_time": "2020-09-15T04:46:26.866960Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initial learning rate\n",
    "INITIAL_LR = 0.1\n",
    "lr_list=[1.0, 0.1, 0.05, 0.02, 0.01, 0.005, 0.002, 0.001]\n",
    "wd_list=[0.01, 0.001, 0.0001, 0.00001, 0.0]\n",
    "# Momentum for optimizer.\n",
    "MOMENTUM = 0.9\n",
    "# Regularization\n",
    "REG = 1e-4\n",
    "# Total number of training epochs\n",
    "EPOCHS = 200\n",
    "# Learning rate decay policy.\n",
    "DECAY_EPOCHS = 50\n",
    "DECAY = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling weights load/save protocols.\n",
    "This handles the weight loading/saving protocols.You may be able to load from checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-15T04:46:26.907164Z",
     "start_time": "2020-09-15T04:46:26.886013Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Training from scratch ...\nStarting from learning rate 0.100000:\n"
    }
   ],
   "source": [
    "CHECKPOINT_PATH = \"./saved_model\"\n",
    "# FLAG for loading the pretrained model\n",
    "TRAIN_FROM_SCRATCH = True\n",
    "# Code for loading checkpoint and recover epoch id.\n",
    "CKPT_PATH = \"./saved_model/model.h5\"\n",
    "def get_checkpoint(ckpt_path):\n",
    "    try:\n",
    "        ckpt = torch.load(ckpt_path)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    return ckpt\n",
    "\n",
    "ckpt = get_checkpoint(CKPT_PATH)\n",
    "if ckpt is None or TRAIN_FROM_SCRATCH:\n",
    "    if not TRAIN_FROM_SCRATCH:\n",
    "        print(\"Checkpoint not found.\")\n",
    "    print(\"Training from scratch ...\")\n",
    "    start_epoch = 0\n",
    "    current_learning_rate = INITIAL_LR\n",
    "else:\n",
    "    print(\"Successfully loaded checkpoint: %s\" %CKPT_PATH)\n",
    "    net.load_state_dict(ckpt['net'])\n",
    "    start_epoch = ckpt['epoch'] + 1\n",
    "    current_learning_rate = ckpt['lr']\n",
    "    print(\"Starting from epoch %d \" %start_epoch)\n",
    "\n",
    "print(\"Starting from learning rate %f:\" %current_learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 Setting up loss functions and Optimizers\n",
    "Loss function is your objective to train the neural networks. Typically, we use multi-class cross entropy as objectives for classification models (e.g., CIFAR-10, MNIST). In this homework, we use SGD optimizer with momentum as our optimizer. You need to formulate the cross-entropy loss function in PyTorch.\n",
    "You should also specify a PyTorch Optimizer to optimize this loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-15T04:46:26.921607Z",
     "start_time": "2020-09-15T04:46:26.914158Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create loss function and specify regularization\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Add optimizer\n",
    "# optimizer = optim.Adam(params=net.parameters(), lr=current_learning_rate, weight_decay=REG)\n",
    "optimizer = optim.SGD(params=net.parameters(), lr=current_learning_rate, momentum=MOMENTUM, weight_decay=REG, nesterov=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Start the training process.\n",
    "Congratulations! You have completed all of the previous steps and it is time to train our neural network.\n",
    "Training a neural network usually composes the following 3 parts: \n",
    "\n",
    "**i) Get a batch of data from the dataloader and copy it to your device (GPU)**\n",
    "\n",
    "**ii) Do a forward pass to get the output logits from the neural network. Compute the forward loss.**\n",
    "\n",
    "**iii) Do a backward pass (back-propagation) to compute gradients of all weights with respect to the loss.**\n",
    "\n",
    "You will also need to compute accuracy within all these parts to justify that your model is doing well on the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-15T05:00:07.585362Z",
     "start_time": "2020-09-15T04:46:26.927623Z"
    },
    "scrolled": false,
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "ing loss: 0.0670, Training accuracy: 0.9772\n2020-09-20 13:09:36.148455\nValidation...\nValidation loss: 0.3886, Validation accuracy: 0.8930\n2020-09-20 13:09:37.122749\nEpoch 91:\nTraining loss: 0.0652, Training accuracy: 0.9777\n2020-09-20 13:09:49.179070\nValidation...\nValidation loss: 0.3635, Validation accuracy: 0.8992\n2020-09-20 13:09:50.114250\nEpoch 92:\nTraining loss: 0.0668, Training accuracy: 0.9773\n2020-09-20 13:10:02.204546\nValidation...\nValidation loss: 0.3775, Validation accuracy: 0.8962\n2020-09-20 13:10:03.163670\nEpoch 93:\nTraining loss: 0.0643, Training accuracy: 0.9776\n2020-09-20 13:10:15.258326\nValidation...\nValidation loss: 0.3959, Validation accuracy: 0.8948\n2020-09-20 13:10:16.173556\nEpoch 94:\nTraining loss: 0.0608, Training accuracy: 0.9796\n2020-09-20 13:10:28.205879\nValidation...\nValidation loss: 0.3850, Validation accuracy: 0.8970\n2020-09-20 13:10:29.138843\nEpoch 95:\nTraining loss: 0.0621, Training accuracy: 0.9782\n2020-09-20 13:10:41.244591\nValidation...\nValidation loss: 0.3713, Validation accuracy: 0.8950\n2020-09-20 13:10:42.150392\nEpoch 96:\nTraining loss: 0.0601, Training accuracy: 0.9801\n2020-09-20 13:10:54.237060\nValidation...\nValidation loss: 0.3853, Validation accuracy: 0.8988\n2020-09-20 13:10:55.152417\nEpoch 97:\nTraining loss: 0.0618, Training accuracy: 0.9782\n2020-09-20 13:11:07.184934\nValidation...\nValidation loss: 0.3694, Validation accuracy: 0.8998\n2020-09-20 13:11:08.101247\nEpoch 98:\nTraining loss: 0.0596, Training accuracy: 0.9799\n2020-09-20 13:11:20.152336\nValidation...\nValidation loss: 0.3764, Validation accuracy: 0.8958\n2020-09-20 13:11:21.064815\nEpoch 99:\nTraining loss: 0.0594, Training accuracy: 0.9798\n2020-09-20 13:11:33.241298\nValidation...\nValidation loss: 0.4085, Validation accuracy: 0.8894\n2020-09-20 13:11:34.167297\nEpoch 100:\nTraining loss: 0.0609, Training accuracy: 0.9785\n2020-09-20 13:11:46.277690\nValidation...\nValidation loss: 0.3927, Validation accuracy: 0.8954\nCurrent learning rate has decayed to 0.001000\n2020-09-20 13:11:47.199071\nEpoch 101:\nTraining loss: 0.0529, Training accuracy: 0.9830\n2020-09-20 13:11:59.364758\nValidation...\nValidation loss: 0.3893, Validation accuracy: 0.8966\n2020-09-20 13:12:00.291340\nEpoch 102:\nTraining loss: 0.0472, Training accuracy: 0.9848\n2020-09-20 13:12:12.402380\nValidation...\nValidation loss: 0.3779, Validation accuracy: 0.9028\n2020-09-20 13:12:13.343493\nEpoch 103:\nTraining loss: 0.0478, Training accuracy: 0.9842\n2020-09-20 13:12:25.502415\nValidation...\nValidation loss: 0.3815, Validation accuracy: 0.8964\n2020-09-20 13:12:26.457070\nEpoch 104:\nTraining loss: 0.0451, Training accuracy: 0.9855\n2020-09-20 13:12:38.504995\nValidation...\nValidation loss: 0.3906, Validation accuracy: 0.8986\n2020-09-20 13:12:39.442710\nEpoch 105:\nTraining loss: 0.0435, Training accuracy: 0.9862\n2020-09-20 13:12:51.532252\nValidation...\nValidation loss: 0.3902, Validation accuracy: 0.8978\n2020-09-20 13:12:52.474306\nEpoch 106:\nTraining loss: 0.0437, Training accuracy: 0.9860\n2020-09-20 13:13:04.583108\nValidation...\nValidation loss: 0.3734, Validation accuracy: 0.8972\n2020-09-20 13:13:05.511358\nEpoch 107:\nTraining loss: 0.0428, Training accuracy: 0.9854\n2020-09-20 13:13:17.608616\nValidation...\nValidation loss: 0.3784, Validation accuracy: 0.9006\n2020-09-20 13:13:18.536041\nEpoch 108:\nTraining loss: 0.0447, Training accuracy: 0.9851\n2020-09-20 13:13:30.649480\nValidation...\nValidation loss: 0.3772, Validation accuracy: 0.8954\n2020-09-20 13:13:31.568574\nEpoch 109:\nTraining loss: 0.0416, Training accuracy: 0.9867\n2020-09-20 13:13:43.591212\nValidation...\nValidation loss: 0.4022, Validation accuracy: 0.8958\n2020-09-20 13:13:44.538686\nEpoch 110:\nTraining loss: 0.0441, Training accuracy: 0.9856\n2020-09-20 13:13:56.599504\nValidation...\nValidation loss: 0.3865, Validation accuracy: 0.8950\n2020-09-20 13:13:57.532740\nEpoch 111:\nTraining loss: 0.0407, Training accuracy: 0.9866\n2020-09-20 13:14:09.610794\nValidation...\nValidation loss: 0.3708, Validation accuracy: 0.9030\n2020-09-20 13:14:10.532705\nEpoch 112:\nTraining loss: 0.0434, Training accuracy: 0.9858\n2020-09-20 13:14:22.630159\nValidation...\nValidation loss: 0.3776, Validation accuracy: 0.8996\n2020-09-20 13:14:23.583725\nEpoch 113:\nTraining loss: 0.0428, Training accuracy: 0.9862\n2020-09-20 13:14:35.702328\nValidation...\nValidation loss: 0.3712, Validation accuracy: 0.8992\n2020-09-20 13:14:36.614775\nEpoch 114:\nTraining loss: 0.0412, Training accuracy: 0.9873\n2020-09-20 13:14:48.620792\nValidation...\nValidation loss: 0.3711, Validation accuracy: 0.8990\n2020-09-20 13:14:49.558943\nEpoch 115:\nTraining loss: 0.0402, Training accuracy: 0.9873\n2020-09-20 13:15:01.750787\nValidation...\nValidation loss: 0.4139, Validation accuracy: 0.9006\n2020-09-20 13:15:02.717951\nEpoch 116:\nTraining loss: 0.0406, Training accuracy: 0.9872\n2020-09-20 13:15:14.813361\nValidation...\nValidation loss: 0.3835, Validation accuracy: 0.8992\n2020-09-20 13:15:15.744448\nEpoch 117:\nTraining loss: 0.0400, Training accuracy: 0.9872\n2020-09-20 13:15:27.991119\nValidation...\nValidation loss: 0.3904, Validation accuracy: 0.8952\n2020-09-20 13:15:28.938622\nEpoch 118:\nTraining loss: 0.0398, Training accuracy: 0.9875\n2020-09-20 13:15:41.079854\nValidation...\nValidation loss: 0.3806, Validation accuracy: 0.8998\n2020-09-20 13:15:41.986009\nEpoch 119:\nTraining loss: 0.0382, Training accuracy: 0.9880\n2020-09-20 13:15:54.024998\nValidation...\nValidation loss: 0.3963, Validation accuracy: 0.8970\n2020-09-20 13:15:54.952505\nEpoch 120:\nTraining loss: 0.0397, Training accuracy: 0.9875\n2020-09-20 13:16:06.996854\nValidation...\nValidation loss: 0.3920, Validation accuracy: 0.8956\n2020-09-20 13:16:07.941363\nEpoch 121:\nTraining loss: 0.0403, Training accuracy: 0.9860\n2020-09-20 13:16:20.056714\nValidation...\nValidation loss: 0.3827, Validation accuracy: 0.9008\n2020-09-20 13:16:20.997668\nEpoch 122:\nTraining loss: 0.0392, Training accuracy: 0.9875\n2020-09-20 13:16:33.111979\nValidation...\nValidation loss: 0.3753, Validation accuracy: 0.9042\nSaving ...\n2020-09-20 13:16:34.021983\nEpoch 123:\nTraining loss: 0.0385, Training accuracy: 0.9878\n2020-09-20 13:16:46.208427\nValidation...\nValidation loss: 0.3907, Validation accuracy: 0.8988\n2020-09-20 13:16:47.126849\nEpoch 124:\nTraining loss: 0.0371, Training accuracy: 0.9883\n2020-09-20 13:16:59.280441\nValidation...\nValidation loss: 0.3993, Validation accuracy: 0.8906\n2020-09-20 13:17:00.200485\nEpoch 125:\nTraining loss: 0.0366, Training accuracy: 0.9889\n2020-09-20 13:17:12.408120\nValidation...\nValidation loss: 0.3986, Validation accuracy: 0.8986\n2020-09-20 13:17:13.349107\nEpoch 126:\nTraining loss: 0.0385, Training accuracy: 0.9879\n2020-09-20 13:17:25.389677\nValidation...\nValidation loss: 0.3958, Validation accuracy: 0.8976\n2020-09-20 13:17:26.348845\nEpoch 127:\nTraining loss: 0.0376, Training accuracy: 0.9885\n2020-09-20 13:17:38.324436\nValidation...\nValidation loss: 0.3970, Validation accuracy: 0.8992\n2020-09-20 13:17:39.276154\nEpoch 128:\nTraining loss: 0.0389, Training accuracy: 0.9876\n2020-09-20 13:17:51.317534\nValidation...\nValidation loss: 0.3861, Validation accuracy: 0.8976\n2020-09-20 13:17:52.264443\nEpoch 129:\nTraining loss: 0.0354, Training accuracy: 0.9886\n2020-09-20 13:18:04.318575\nValidation...\nValidation loss: 0.3833, Validation accuracy: 0.8988\n2020-09-20 13:18:05.226584\nEpoch 130:\nTraining loss: 0.0364, Training accuracy: 0.9886\n2020-09-20 13:18:17.279400\nValidation...\nValidation loss: 0.3993, Validation accuracy: 0.8996\n2020-09-20 13:18:18.231557\nEpoch 131:\nTraining loss: 0.0368, Training accuracy: 0.9885\n2020-09-20 13:18:30.227977\nValidation...\nValidation loss: 0.3724, Validation accuracy: 0.9008\n2020-09-20 13:18:31.171188\nEpoch 132:\nTraining loss: 0.0362, Training accuracy: 0.9887\n2020-09-20 13:18:42.541730\nValidation...\nValidation loss: 0.3845, Validation accuracy: 0.8996\n2020-09-20 13:18:43.411658\nEpoch 133:\nTraining loss: 0.0374, Training accuracy: 0.9881\n2020-09-20 13:18:54.749562\nValidation...\nValidation loss: 0.3837, Validation accuracy: 0.9006\n2020-09-20 13:18:55.663430\nEpoch 134:\nTraining loss: 0.0356, Training accuracy: 0.9890\n2020-09-20 13:19:07.780601\nValidation...\nValidation loss: 0.3790, Validation accuracy: 0.9002\n2020-09-20 13:19:08.721938\nEpoch 135:\nTraining loss: 0.0366, Training accuracy: 0.9885\n2020-09-20 13:19:20.711826\nValidation...\nValidation loss: 0.3895, Validation accuracy: 0.8984\n2020-09-20 13:19:21.682294\nEpoch 136:\nTraining loss: 0.0364, Training accuracy: 0.9885\n2020-09-20 13:19:33.943876\nValidation...\nValidation loss: 0.3964, Validation accuracy: 0.8956\n2020-09-20 13:19:34.856986\nEpoch 137:\nTraining loss: 0.0343, Training accuracy: 0.9900\n2020-09-20 13:19:46.976935\nValidation...\nValidation loss: 0.3831, Validation accuracy: 0.8996\n2020-09-20 13:19:47.911308\nEpoch 138:\nTraining loss: 0.0359, Training accuracy: 0.9885\n2020-09-20 13:19:59.948457\nValidation...\nValidation loss: 0.3930, Validation accuracy: 0.8948\n2020-09-20 13:20:00.890692\nEpoch 139:\nTraining loss: 0.0383, Training accuracy: 0.9876\n2020-09-20 13:20:12.959121\nValidation...\nValidation loss: 0.3803, Validation accuracy: 0.8954\n2020-09-20 13:20:13.860896\nEpoch 140:\nTraining loss: 0.0367, Training accuracy: 0.9889\n2020-09-20 13:20:25.925971\nValidation...\nValidation loss: 0.3974, Validation accuracy: 0.8928\n2020-09-20 13:20:26.800426\nEpoch 141:\nTraining loss: 0.0356, Training accuracy: 0.9891\n2020-09-20 13:20:38.863530\nValidation...\nValidation loss: 0.3906, Validation accuracy: 0.9006\n2020-09-20 13:20:39.816035\nEpoch 142:\nTraining loss: 0.0355, Training accuracy: 0.9887\n2020-09-20 13:20:51.960955\nValidation...\nValidation loss: 0.3772, Validation accuracy: 0.9002\n2020-09-20 13:20:52.878119\nEpoch 143:\nTraining loss: 0.0341, Training accuracy: 0.9895\n2020-09-20 13:21:04.974205\nValidation...\nValidation loss: 0.3982, Validation accuracy: 0.8954\n2020-09-20 13:21:05.909487\nEpoch 144:\nTraining loss: 0.0352, Training accuracy: 0.9889\n2020-09-20 13:21:18.005786\nValidation...\nValidation loss: 0.3918, Validation accuracy: 0.8994\n2020-09-20 13:21:18.973817\nEpoch 145:\nTraining loss: 0.0352, Training accuracy: 0.9891\n2020-09-20 13:21:31.160085\nValidation...\nValidation loss: 0.3997, Validation accuracy: 0.8996\n2020-09-20 13:21:32.060599\nEpoch 146:\nTraining loss: 0.0358, Training accuracy: 0.9886\n2020-09-20 13:21:44.089175\nValidation...\nValidation loss: 0.4055, Validation accuracy: 0.8976\n2020-09-20 13:21:45.000996\nEpoch 147:\nTraining loss: 0.0342, Training accuracy: 0.9890\n2020-09-20 13:21:57.015648\nValidation...\nValidation loss: 0.3840, Validation accuracy: 0.9020\n2020-09-20 13:21:57.974237\nEpoch 148:\nTraining loss: 0.0340, Training accuracy: 0.9894\n2020-09-20 13:22:10.053901\nValidation...\nValidation loss: 0.3993, Validation accuracy: 0.8976\n2020-09-20 13:22:11.002013\nEpoch 149:\nTraining loss: 0.0339, Training accuracy: 0.9889\n2020-09-20 13:22:23.193126\nValidation...\nValidation loss: 0.3893, Validation accuracy: 0.8990\n2020-09-20 13:22:24.129449\nEpoch 150:\nTraining loss: 0.0347, Training accuracy: 0.9894\n2020-09-20 13:22:36.209673\nValidation...\nValidation loss: 0.3919, Validation accuracy: 0.8976\nCurrent learning rate has decayed to 0.000100\n2020-09-20 13:22:37.166654\nEpoch 151:\nTraining loss: 0.0349, Training accuracy: 0.9900\n2020-09-20 13:22:49.274931\nValidation...\nValidation loss: 0.3935, Validation accuracy: 0.8986\n2020-09-20 13:22:50.216474\nEpoch 152:\nTraining loss: 0.0370, Training accuracy: 0.9878\n2020-09-20 13:23:02.293787\nValidation...\nValidation loss: 0.3838, Validation accuracy: 0.8966\n2020-09-20 13:23:03.230845\nEpoch 153:\nTraining loss: 0.0329, Training accuracy: 0.9900\n2020-09-20 13:23:15.355484\nValidation...\nValidation loss: 0.3979, Validation accuracy: 0.8930\n2020-09-20 13:23:16.309784\nEpoch 154:\nTraining loss: 0.0345, Training accuracy: 0.9894\n2020-09-20 13:23:28.403956\nValidation...\nValidation loss: 0.3923, Validation accuracy: 0.8958\n2020-09-20 13:23:29.316535\nEpoch 155:\nTraining loss: 0.0327, Training accuracy: 0.9895\n2020-09-20 13:23:41.402877\nValidation...\nValidation loss: 0.4062, Validation accuracy: 0.8938\n2020-09-20 13:23:42.317645\nEpoch 156:\nTraining loss: 0.0332, Training accuracy: 0.9897\n2020-09-20 13:23:54.480979\nValidation...\nValidation loss: 0.4058, Validation accuracy: 0.8998\n2020-09-20 13:23:55.419690\nEpoch 157:\nTraining loss: 0.0342, Training accuracy: 0.9894\n2020-09-20 13:24:07.451417\nValidation...\nValidation loss: 0.4158, Validation accuracy: 0.8946\n2020-09-20 13:24:08.365339\nEpoch 158:\nTraining loss: 0.0342, Training accuracy: 0.9895\n2020-09-20 13:24:20.477913\nValidation...\nValidation loss: 0.3809, Validation accuracy: 0.8972\n2020-09-20 13:24:21.437252\nEpoch 159:\nTraining loss: 0.0323, Training accuracy: 0.9903\n2020-09-20 13:24:33.546987\nValidation...\nValidation loss: 0.3799, Validation accuracy: 0.8998\n2020-09-20 13:24:34.495226\nEpoch 160:\nTraining loss: 0.0328, Training accuracy: 0.9898\n2020-09-20 13:24:46.555240\nValidation...\nValidation loss: 0.3920, Validation accuracy: 0.8960\n2020-09-20 13:24:47.455646\nEpoch 161:\nTraining loss: 0.0341, Training accuracy: 0.9891\n2020-09-20 13:24:59.518078\nValidation...\nValidation loss: 0.3826, Validation accuracy: 0.9016\n2020-09-20 13:25:00.454420\nEpoch 162:\nTraining loss: 0.0353, Training accuracy: 0.9884\n2020-09-20 13:25:12.581980\nValidation...\nValidation loss: 0.3817, Validation accuracy: 0.9020\n2020-09-20 13:25:13.526628\nEpoch 163:\nTraining loss: 0.0335, Training accuracy: 0.9897\n2020-09-20 13:25:25.630501\nValidation...\nValidation loss: 0.4018, Validation accuracy: 0.8944\n2020-09-20 13:25:26.537832\nEpoch 164:\nTraining loss: 0.0337, Training accuracy: 0.9895\n2020-09-20 13:25:38.611595\nValidation...\nValidation loss: 0.3942, Validation accuracy: 0.8964\n2020-09-20 13:25:39.567879\nEpoch 165:\nTraining loss: 0.0341, Training accuracy: 0.9897\n2020-09-20 13:25:51.711870\nValidation...\nValidation loss: 0.4010, Validation accuracy: 0.8954\n2020-09-20 13:25:52.649506\nEpoch 166:\nTraining loss: 0.0331, Training accuracy: 0.9902\n2020-09-20 13:26:04.818798\nValidation...\nValidation loss: 0.4098, Validation accuracy: 0.8934\n2020-09-20 13:26:05.721862\nEpoch 167:\nTraining loss: 0.0322, Training accuracy: 0.9900\n2020-09-20 13:26:17.786554\nValidation...\nValidation loss: 0.4071, Validation accuracy: 0.8950\n2020-09-20 13:26:18.730082\nEpoch 168:\nTraining loss: 0.0320, Training accuracy: 0.9902\n2020-09-20 13:26:30.738704\nValidation...\nValidation loss: 0.4074, Validation accuracy: 0.8956\n2020-09-20 13:26:31.666981\nEpoch 169:\nTraining loss: 0.0327, Training accuracy: 0.9896\n2020-09-20 13:26:43.790040\nValidation...\nValidation loss: 0.4029, Validation accuracy: 0.8964\n2020-09-20 13:26:44.704716\nEpoch 170:\nTraining loss: 0.0347, Training accuracy: 0.9894\n2020-09-20 13:26:56.735303\nValidation...\nValidation loss: 0.4019, Validation accuracy: 0.8936\n2020-09-20 13:26:57.683790\nEpoch 171:\nTraining loss: 0.0322, Training accuracy: 0.9909\n2020-09-20 13:27:09.876810\nValidation...\nValidation loss: 0.3806, Validation accuracy: 0.8994\n2020-09-20 13:27:10.817776\nEpoch 172:\nTraining loss: 0.0333, Training accuracy: 0.9897\n2020-09-20 13:27:22.910796\nValidation...\nValidation loss: 0.4018, Validation accuracy: 0.8972\n2020-09-20 13:27:23.837671\nEpoch 173:\nTraining loss: 0.0351, Training accuracy: 0.9888\n2020-09-20 13:27:36.035243\nValidation...\nValidation loss: 0.4066, Validation accuracy: 0.8998\n2020-09-20 13:27:36.953332\nEpoch 174:\nTraining loss: 0.0333, Training accuracy: 0.9899\n2020-09-20 13:27:49.019120\nValidation...\nValidation loss: 0.3994, Validation accuracy: 0.8992\n2020-09-20 13:27:49.922759\nEpoch 175:\nTraining loss: 0.0323, Training accuracy: 0.9898\n2020-09-20 13:28:02.023428\nValidation...\nValidation loss: 0.3799, Validation accuracy: 0.8988\n2020-09-20 13:28:02.969740\nEpoch 176:\nTraining loss: 0.0326, Training accuracy: 0.9903\n2020-09-20 13:28:15.063656\nValidation...\nValidation loss: 0.3895, Validation accuracy: 0.8958\n2020-09-20 13:28:15.967915\nEpoch 177:\nTraining loss: 0.0333, Training accuracy: 0.9898\n2020-09-20 13:28:27.969520\nValidation...\nValidation loss: 0.3869, Validation accuracy: 0.9006\n2020-09-20 13:28:28.894854\nEpoch 178:\nTraining loss: 0.0328, Training accuracy: 0.9899\n2020-09-20 13:28:40.895839\nValidation...\nValidation loss: 0.4036, Validation accuracy: 0.9000\n2020-09-20 13:28:41.823610\nEpoch 179:\nTraining loss: 0.0331, Training accuracy: 0.9899\n2020-09-20 13:28:53.945064\nValidation...\nValidation loss: 0.3889, Validation accuracy: 0.9030\n2020-09-20 13:28:54.844198\nEpoch 180:\nTraining loss: 0.0337, Training accuracy: 0.9892\n2020-09-20 13:29:06.916630\nValidation...\nValidation loss: 0.4120, Validation accuracy: 0.8952\n2020-09-20 13:29:07.847991\nEpoch 181:\nTraining loss: 0.0340, Training accuracy: 0.9891\n2020-09-20 13:29:20.059080\nValidation...\nValidation loss: 0.3978, Validation accuracy: 0.8950\n2020-09-20 13:29:20.972417\nEpoch 182:\nTraining loss: 0.0348, Training accuracy: 0.9892\n2020-09-20 13:29:33.026727\nValidation...\nValidation loss: 0.4020, Validation accuracy: 0.8962\n2020-09-20 13:29:33.979576\nEpoch 183:\nTraining loss: 0.0328, Training accuracy: 0.9901\n2020-09-20 13:29:46.201925\nValidation...\nValidation loss: 0.3998, Validation accuracy: 0.9006\n2020-09-20 13:29:47.147610\nEpoch 184:\nTraining loss: 0.0328, Training accuracy: 0.9903\n2020-09-20 13:29:59.159998\nValidation...\nValidation loss: 0.3952, Validation accuracy: 0.9000\n2020-09-20 13:30:00.113940\nEpoch 185:\nTraining loss: 0.0347, Training accuracy: 0.9891\n2020-09-20 13:30:12.204312\nValidation...\nValidation loss: 0.4047, Validation accuracy: 0.8922\n2020-09-20 13:30:13.155246\nEpoch 186:\nTraining loss: 0.0338, Training accuracy: 0.9896\n2020-09-20 13:30:25.153285\nValidation...\nValidation loss: 0.3945, Validation accuracy: 0.8984\n2020-09-20 13:30:26.095539\nEpoch 187:\nTraining loss: 0.0314, Training accuracy: 0.9899\n2020-09-20 13:30:38.159023\nValidation...\nValidation loss: 0.3998, Validation accuracy: 0.8936\n2020-09-20 13:30:39.084418\nEpoch 188:\nTraining loss: 0.0315, Training accuracy: 0.9904\n2020-09-20 13:30:51.200449\nValidation...\nValidation loss: 0.3946, Validation accuracy: 0.8976\n2020-09-20 13:30:52.109451\nEpoch 189:\nTraining loss: 0.0344, Training accuracy: 0.9889\n2020-09-20 13:31:04.296858\nValidation...\nValidation loss: 0.3800, Validation accuracy: 0.8932\n2020-09-20 13:31:05.226165\nEpoch 190:\nTraining loss: 0.0336, Training accuracy: 0.9893\n2020-09-20 13:31:17.266591\nValidation...\nValidation loss: 0.3841, Validation accuracy: 0.8978\n2020-09-20 13:31:18.169579\nEpoch 191:\nTraining loss: 0.0325, Training accuracy: 0.9900\n2020-09-20 13:31:30.285751\nValidation...\nValidation loss: 0.3935, Validation accuracy: 0.8984\n2020-09-20 13:31:31.224250\nEpoch 192:\nTraining loss: 0.0313, Training accuracy: 0.9904\n2020-09-20 13:31:43.265654\nValidation...\nValidation loss: 0.3904, Validation accuracy: 0.9020\n2020-09-20 13:31:44.188380\nEpoch 193:\nTraining loss: 0.0336, Training accuracy: 0.9893\n2020-09-20 13:31:56.287828\nValidation...\nValidation loss: 0.4107, Validation accuracy: 0.8990\n2020-09-20 13:31:57.233439\nEpoch 194:\nTraining loss: 0.0339, Training accuracy: 0.9890\n2020-09-20 13:32:09.352625\nValidation...\nValidation loss: 0.4072, Validation accuracy: 0.8972\n2020-09-20 13:32:10.317804\nEpoch 195:\nTraining loss: 0.0328, Training accuracy: 0.9898\n2020-09-20 13:32:22.590466\nValidation...\nValidation loss: 0.3812, Validation accuracy: 0.8994\n2020-09-20 13:32:23.569308\nEpoch 196:\nTraining loss: 0.0305, Training accuracy: 0.9912\n2020-09-20 13:32:35.740060\nValidation...\nValidation loss: 0.3870, Validation accuracy: 0.9012\n2020-09-20 13:32:36.643583\nEpoch 197:\nTraining loss: 0.0315, Training accuracy: 0.9905\n2020-09-20 13:32:48.658795\nValidation...\nValidation loss: 0.3876, Validation accuracy: 0.8984\n2020-09-20 13:32:49.566351\nEpoch 198:\nTraining loss: 0.0327, Training accuracy: 0.9897\n2020-09-20 13:33:01.801178\nValidation...\nValidation loss: 0.3907, Validation accuracy: 0.8960\n2020-09-20 13:33:02.734565\nEpoch 199:\nTraining loss: 0.0340, Training accuracy: 0.9889\n2020-09-20 13:33:14.817782\nValidation...\nValidation loss: 0.3910, Validation accuracy: 0.9010\nOptimization finished.\nbest val 0.9042\n"
    }
   ],
   "source": [
    "# Start the training/validation process\n",
    "# The process should take about 5 minutes on a GTX 1070-Ti\n",
    "# if the code is written efficiently.\n",
    "from    copy import deepcopy\n",
    "global_step = 0\n",
    "best_val_acc = 0\n",
    "bestnet = deepcopy(net)\n",
    "for i in range(start_epoch, EPOCHS):\n",
    "    print(datetime.datetime.now())\n",
    "    # Switch to train mode\n",
    "    net.train()\n",
    "    print(\"Epoch %d:\" %i)\n",
    "\n",
    "    total_examples = 0\n",
    "    correct_examples = 0\n",
    "\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    \n",
    "    # Train the training dataset for 1 epoch.\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        # Copy inputs to device\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        # Zero the gradient\n",
    "        optimizer.zero_grad()\n",
    "        # Generate output\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "#         if i==0:\n",
    "#             print('initial loss: ',loss)\n",
    "        # Now backward loss\n",
    "        loss.backward()\n",
    "        # Apply gradient\n",
    "        optimizer.step()\n",
    "        # Calculate predicted labels\n",
    "        _, predicted = outputs.max(1)\n",
    "        total_examples += predicted.size(0)\n",
    "        correct_examples += predicted.eq(targets).sum().item()\n",
    "        train_loss += loss\n",
    "        global_step += 1\n",
    "                \n",
    "    avg_loss = train_loss / (batch_idx + 1)\n",
    "    avg_acc = correct_examples / total_examples\n",
    "    print(\"Training loss: %.4f, Training accuracy: %.4f\" %(avg_loss, avg_acc))\n",
    "    print(datetime.datetime.now())\n",
    "    # Validate on the validation dataset\n",
    "    print(\"Validation...\")\n",
    "    total_examples = 0\n",
    "    correct_examples = 0\n",
    "    \n",
    "    net.eval()\n",
    "\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    # Disable gradient during validation\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(valloader):\n",
    "            # Copy inputs to device\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            # Zero the gradient\n",
    "            optimizer.zero_grad()\n",
    "            # Generate output from the DNN.\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)            \n",
    "            # Calculate predicted labels\n",
    "            _, predicted = outputs.max(1)\n",
    "            total_examples += predicted.size(0)\n",
    "            correct_examples += predicted.eq(targets).sum().item()\n",
    "            val_loss += loss\n",
    "\n",
    "    avg_loss = val_loss / len(valloader)\n",
    "    avg_acc = correct_examples / total_examples\n",
    "    \n",
    "    print(\"Validation loss: %.4f, Validation accuracy: %.4f\" % (avg_loss, avg_acc))\n",
    "\n",
    "    # Handle the learning rate scheduler.\n",
    "    if i % DECAY_EPOCHS == 0 and i != 0:\n",
    "        current_learning_rate = current_learning_rate * DECAY\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = current_learning_rate\n",
    "        print(\"Current learning rate has decayed to %f\" %current_learning_rate)\n",
    "    \n",
    "    # Save for checkpoint\n",
    "    if avg_acc > best_val_acc:\n",
    "        best_val_acc = avg_acc\n",
    "        bestnet = deepcopy(net)\n",
    "        if not os.path.exists(CHECKPOINT_PATH):\n",
    "            os.makedirs(CHECKPOINT_PATH)\n",
    "        print(\"Saving ...\")\n",
    "        state = {'net': net.state_dict(),\n",
    "                 'epoch': i,\n",
    "                 'lr': current_learning_rate}\n",
    "        torch.save(state, os.path.join(CHECKPOINT_PATH, 'model.h5'))\n",
    "\n",
    "print(\"Optimization finished.\")\n",
    "print(\"best val\",best_val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Using downloaded and verified file: ./data/cifar10_test_F20.zip\nExtracting ./data/cifar10_test_F20.zip to ./data\nFiles already downloaded and verified\nTesting dataset has 10000 examples!\n"
    }
   ],
   "source": [
    "from tools.dataloader_test import CIFAR10\n",
    "# Call the dataset Loader\n",
    "DATAROOT = \"./data\"\n",
    "TRAIN_BATCH_SIZE = 128\n",
    "VAL_BATCH_SIZE = 100\n",
    "testset = CIFAR10(root=DATAROOT, train=False, download=True, transform=transform_val)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False, num_workers=4)\n",
    "\n",
    "prediction = []\n",
    "with torch.no_grad():\n",
    "\n",
    "    for batch_idx, (inputs,_) in enumerate(testloader):\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = net(inputs)\n",
    "        pred = torch.argmax(outputs)\n",
    "        prediction.append(pred.item())\n",
    "\n",
    "f = open(\"./prediction.csv\", \"w\")\n",
    "f.write(\"Id,Label\\n\")\n",
    "for i in range(len(prediction )):\n",
    "  f.write(\"{0},{1}\\n\".format(i, prediction[i]))\n",
    "f.close()\n",
    " \n",
    "\n",
    "\n",
    "# print(pred_lab[0])\n",
    "# CKPT_PATH = \"./saved_model/model.h5\"\n",
    "# best = torch.load(CKPT_PATH)\n",
    "# for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "#         # Copy inputs to device\n",
    "#         inputs = inputs.to(device)\n",
    "#         targets = targets.to(device)\n",
    "#         # Zero the gradient\n",
    "#         # Generate output\n",
    "#         outputs = best(inputs)\n",
    "\n",
    "#         loss = criterion(outputs, targets)\n",
    "# #         if i==0:\n",
    "# #             print('initial loss: ',loss)\n",
    "#         # Now backward loss\n",
    "#         # Apply gradient\n",
    "#         # Calculate predicted labels\n",
    "#         _, predicted = outputs.max(1)\n",
    "#         total_examples += predicted.size(0)\n",
    "#         correct_examples += predicted.eq(targets).sum().item()\n",
    "#         print(correct_examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "2.7.12-final"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}